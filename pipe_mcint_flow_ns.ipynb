{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from model.nets import Siren\n",
    "from model.flow import PipeFlow\n",
    "from model.integrator import Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wo = 5\n",
    "Pn = [  complex(-0.5060,0.1245),\n",
    "        complex(0.0802,0.0886),\n",
    "        complex(0.0591,0.0251),\n",
    "        complex(0.0228,0.0244),\n",
    "        complex(0.0141,0.0071),\n",
    "        complex(0.0112,0.0107),\n",
    "        complex(0.0065,0.0056),\n",
    "        complex(0.0067,0.0050),\n",
    "        complex(0.0030,0.0040),\n",
    "        complex(0.0044,0.0030),\n",
    "        complex(0.0000,0.3710),\n",
    "        complex(0.1493,0.0754),\n",
    "        complex(0.0161,0.0103),\n",
    "        complex(0.0166,0.0082),\n",
    "        complex(-0.0002,0.0046),\n",
    "        complex(0.0063,0.0001),\n",
    "        complex(0.0008,0.0013),\n",
    "        complex(0.0021,-0.0012),\n",
    "        complex(0.0009,0.0015),\n",
    "        complex(0.0001,-0.0001)]\n",
    "pipe = PipeFlow(Po=-0.05,Pn=Pn,Wo=Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb = np.linspace(0,8,4*Nx)\n",
    "# yb = np.ones_like(xb)\n",
    "# xb = np.concatenate((xb,xb),axis=0)\n",
    "# yb = np.concatenate((yb,yb*-1),axis=0)\n",
    "# tb = np.zeros_like(xb)\n",
    "# ub = np.zeros_like(xb)\n",
    "# vb = np.zeros_like(xb)\n",
    "# dpdyb = np.zeros_like(xb)\n",
    "\n",
    "# Xb = np.concatenate((xb.reshape((8*Nx,1)),yb.reshape((8*Nx,1)),tb.reshape((8*Nx,1))),axis=1)\n",
    "# Ub = np.concatenate((ub.reshape((8*Nx,1)),vb.reshape((8*Nx,1))),axis=1)\n",
    "# Xb = torch.from_numpy(Xb).float().cuda()\n",
    "# Ub = torch.from_numpy(Ub).float().cuda()\n",
    "# dpdyb = torch.from_numpy(dpdyb).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Siren(\n",
       "  (net): Sequential(\n",
       "    (0): SineLayer(\n",
       "      (linear): Linear(in_features=4, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): SineLayer(\n",
       "      (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (2): SineLayer(\n",
       "      (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (3): SineLayer(\n",
       "      (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (4): SineLayer(\n",
       "      (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): SineLayer(\n",
       "      (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (6): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Siren(in_features=4, out_features=4, hidden_features=512, \n",
    "                  hidden_layers=5, outermost_linear=True)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0: 0.028991686180233955\n",
      "Loss 1: 0.10047820210456848\n",
      "Loss 2: 0.05597878247499466\n",
      "Loss 3: 0.06671324372291565\n",
      "Loss 4: 0.06260371208190918\n",
      "Loss 5: 0.05769031494855881\n",
      "Loss 6: 0.05502700060606003\n",
      "Loss 7: 0.05934951826930046\n",
      "Loss 8: 0.05073578655719757\n",
      "Loss 9: 0.047261592000722885\n",
      "Loss 10: 0.05157905071973801\n",
      "Loss 11: 0.05377773940563202\n",
      "Loss 12: 0.04833653196692467\n",
      "Loss 13: 0.046037230640649796\n",
      "Loss 14: 0.04488750919699669\n",
      "Loss 15: 0.04767150431871414\n",
      "Loss 16: 0.04590035602450371\n",
      "Loss 17: 0.044654905796051025\n",
      "Loss 18: 0.04189611226320267\n",
      "Loss 19: 0.041007764637470245\n",
      "Loss 20: 0.041733331978321075\n",
      "Loss 21: 0.040548354387283325\n",
      "Loss 22: 0.040779951959848404\n",
      "Loss 23: 0.0401897206902504\n",
      "Loss 24: 0.039392489939928055\n",
      "Loss 25: 0.03825885429978371\n",
      "Loss 26: 0.03998102992773056\n",
      "Loss 27: 0.03689468652009964\n",
      "Loss 28: 0.03674223646521568\n",
      "Loss 29: 0.03960568830370903\n",
      "Loss 30: 0.037268538028001785\n",
      "Loss 31: 0.03780417889356613\n",
      "Loss 32: 0.03693182393908501\n",
      "Loss 33: 0.03720531612634659\n",
      "Loss 34: 0.03651779890060425\n",
      "Loss 35: 0.03589947894215584\n",
      "Loss 36: 0.035625118762254715\n",
      "Loss 37: 0.03654452785849571\n",
      "Loss 38: 0.03599885106086731\n",
      "Loss 39: 0.03402320668101311\n",
      "Loss 40: 0.03408192843198776\n",
      "Loss 41: 0.035800326615571976\n",
      "Loss 42: 0.0324966125190258\n",
      "Loss 43: 0.03347107395529747\n",
      "Loss 44: 0.03448699042201042\n",
      "Loss 45: 0.03241558000445366\n",
      "Loss 46: 0.0322493351995945\n",
      "Loss 47: 0.0323779322206974\n",
      "Loss 48: 0.03209954500198364\n",
      "Loss 49: 0.032587792724370956\n",
      "Loss 50: 0.035405755043029785\n",
      "Loss 51: 0.031864456832408905\n",
      "Loss 52: 0.03288213908672333\n",
      "Loss 53: 0.032327890396118164\n",
      "Loss 54: 0.03206758201122284\n",
      "Loss 55: 0.032509222626686096\n",
      "Loss 56: 0.033131130039691925\n",
      "Loss 57: 0.03217735141515732\n",
      "Loss 58: 0.03276568278670311\n",
      "Loss 59: 0.03325342759490013\n",
      "Loss 60: 0.03289255499839783\n",
      "Loss 61: 0.032839901745319366\n",
      "Loss 62: 0.03478899598121643\n",
      "Loss 63: 0.032601431012153625\n",
      "Loss 64: 0.03291480243206024\n",
      "Loss 65: 0.03480517864227295\n",
      "Loss 66: 0.03490728512406349\n",
      "Loss 67: 0.03214455023407936\n",
      "Loss 68: 0.03251279518008232\n",
      "Loss 69: 0.033116064965724945\n",
      "Loss 70: 0.032134708017110825\n",
      "Loss 71: 0.033229779452085495\n",
      "Loss 72: 0.03377288207411766\n",
      "Loss 73: 0.03309302404522896\n",
      "Loss 74: 0.03384458273649216\n",
      "Loss 75: 0.03176542744040489\n",
      "Loss 76: 0.03323952481150627\n",
      "Loss 77: 0.03234387934207916\n",
      "Loss 78: 0.03349369019269943\n",
      "Loss 79: 0.0322769470512867\n",
      "Loss 80: 0.03306161239743233\n",
      "Loss 81: 0.031057314947247505\n",
      "Loss 82: 0.03394829481840134\n",
      "Loss 83: 0.03210332617163658\n",
      "Loss 84: 0.03211284801363945\n",
      "Loss 85: 0.03302963450551033\n",
      "Loss 86: 0.03221656754612923\n",
      "Loss 87: 0.03146127238869667\n",
      "Loss 88: 0.03312135115265846\n",
      "Loss 89: 0.030795887112617493\n",
      "Loss 90: 0.030269527807831764\n",
      "Loss 91: 0.03175686299800873\n",
      "Loss 92: 0.03145512938499451\n",
      "Loss 93: 0.03341184929013252\n",
      "Loss 94: 0.032303307205438614\n",
      "Loss 95: 0.03326694667339325\n",
      "Loss 96: 0.03225141763687134\n",
      "Loss 97: 0.03222132846713066\n",
      "Loss 98: 0.032444197684526443\n",
      "Loss 99: 0.03248174116015434\n",
      "Loss 100: 0.031472645699977875\n",
      "Loss 101: 0.030534235760569572\n",
      "Loss 102: 0.03144865110516548\n",
      "Loss 103: 0.03247234970331192\n",
      "Loss 104: 0.03234788030385971\n",
      "Loss 105: 0.03221014887094498\n",
      "Loss 106: 0.0327581986784935\n",
      "Loss 107: 0.03236842155456543\n",
      "Loss 108: 0.03628096356987953\n",
      "Loss 109: 0.03158694878220558\n",
      "Loss 110: 0.03139936178922653\n",
      "Loss 111: 0.03154945373535156\n",
      "Loss 112: 0.0331251285970211\n",
      "Loss 113: 0.034095440059900284\n",
      "Loss 114: 0.033194635063409805\n",
      "Loss 115: 0.03272461146116257\n",
      "Loss 116: 0.03218618407845497\n",
      "Loss 117: 0.03522340580821037\n",
      "Loss 118: 0.03430352732539177\n",
      "Loss 119: 0.033911626785993576\n",
      "Loss 120: 0.03163248300552368\n",
      "Loss 121: 0.03414692357182503\n",
      "Loss 122: 0.03251978009939194\n",
      "Loss 123: 0.03393664211034775\n",
      "Loss 124: 0.032948482781648636\n",
      "Loss 125: 0.03245510905981064\n",
      "Loss 126: 0.03319583088159561\n",
      "Loss 127: 0.033524319529533386\n",
      "Loss 128: 0.03356442600488663\n",
      "Loss 129: 0.03361818566918373\n",
      "Loss 130: 0.032710544764995575\n",
      "Loss 131: 0.03361089900135994\n",
      "Loss 132: 0.033076487481594086\n",
      "Loss 133: 0.0333319716155529\n",
      "Loss 134: 0.03396649658679962\n",
      "Loss 135: 0.03344302251935005\n",
      "Loss 136: 0.032418347895145416\n",
      "Loss 137: 0.03396689519286156\n",
      "Loss 138: 0.033376071602106094\n",
      "Loss 139: 0.03170264512300491\n",
      "Loss 140: 0.033439334481954575\n",
      "Loss 141: 0.03140398859977722\n",
      "Loss 142: 0.03235554322600365\n",
      "Loss 143: 0.03184676542878151\n",
      "Loss 144: 0.03417516499757767\n",
      "Loss 145: 0.03369560465216637\n",
      "Loss 146: 0.03214607760310173\n",
      "Loss 147: 0.03368820622563362\n",
      "Loss 148: 0.032468412071466446\n",
      "Loss 149: 0.032016169279813766\n",
      "Loss 150: 0.031816523522138596\n",
      "Loss 151: 0.030043913051486015\n",
      "Loss 152: 0.03182639181613922\n",
      "Loss 153: 0.03250618278980255\n",
      "Loss 154: 0.03263181447982788\n",
      "Loss 155: 0.031859323382377625\n",
      "Loss 156: 0.03229914605617523\n",
      "Loss 157: 0.03213121369481087\n",
      "Loss 158: 0.031858786940574646\n",
      "Loss 159: 0.03140788897871971\n",
      "Loss 160: 0.03188624233007431\n",
      "Loss 161: 0.033424727618694305\n",
      "Loss 162: 0.03225104883313179\n",
      "Loss 163: 0.03288908302783966\n",
      "Loss 164: 0.03279855474829674\n",
      "Loss 165: 0.031402669847011566\n",
      "Loss 166: 0.03251199051737785\n",
      "Loss 167: 0.03184674680233002\n",
      "Loss 168: 0.03158574551343918\n",
      "Loss 169: 0.03192079812288284\n",
      "Loss 170: 0.032966647297143936\n",
      "Loss 171: 0.03373041749000549\n",
      "Loss 172: 0.03043617308139801\n",
      "Loss 173: 0.03235473856329918\n",
      "Loss 174: 0.03255894407629967\n",
      "Loss 175: 0.0322999581694603\n",
      "Loss 176: 0.03272311016917229\n",
      "Loss 177: 0.03262702003121376\n",
      "Loss 178: 0.03315244987607002\n",
      "Loss 179: 0.030846962705254555\n",
      "Loss 180: 0.031435947865247726\n",
      "Loss 181: 0.03212037310004234\n",
      "Loss 182: 0.03118390217423439\n",
      "Loss 183: 0.032819245010614395\n",
      "Loss 184: 0.03176867589354515\n",
      "Loss 185: 0.031223000958561897\n",
      "Loss 186: 0.029924890026450157\n",
      "Loss 187: 0.03160897269845009\n",
      "Loss 188: 0.030995039269328117\n",
      "Loss 189: 0.031017649918794632\n",
      "Loss 190: 0.03134913370013237\n",
      "Loss 191: 0.03275064378976822\n",
      "Loss 192: 0.03058243915438652\n",
      "Loss 193: 0.031033387407660484\n",
      "Loss 194: 0.031081758439540863\n",
      "Loss 195: 0.031256262212991714\n",
      "Loss 196: 0.03014870174229145\n",
      "Loss 197: 0.03007230907678604\n",
      "Loss 198: 0.03192949295043945\n",
      "Loss 199: 0.030442386865615845\n",
      "Loss 200: 0.030301300808787346\n",
      "Loss 201: 0.029834507033228874\n",
      "Loss 202: 0.03002610057592392\n",
      "Loss 203: 0.03227437287569046\n",
      "Loss 204: 0.02996896207332611\n",
      "Loss 205: 0.03160937502980232\n",
      "Loss 206: 0.030902421101927757\n",
      "Loss 207: 0.030518895015120506\n",
      "Loss 208: 0.031213976442813873\n",
      "Loss 209: 0.03267790377140045\n",
      "Loss 210: 0.031873952597379684\n",
      "Loss 211: 0.03119519352912903\n",
      "Loss 212: 0.03048677369952202\n",
      "Loss 213: 0.032198868691921234\n",
      "Loss 214: 0.032570671290159225\n",
      "Loss 215: 0.03402202948927879\n",
      "Loss 216: 0.032281409949064255\n",
      "Loss 217: 0.0347246453166008\n",
      "Loss 218: 0.03404732793569565\n",
      "Loss 219: 0.03387610986828804\n",
      "Loss 220: 0.03401677682995796\n",
      "Loss 221: 0.03474181145429611\n",
      "Loss 222: 0.03639163821935654\n",
      "Loss 223: 0.034375954419374466\n",
      "Loss 224: 0.03474511206150055\n",
      "Loss 225: 0.035930972546339035\n",
      "Loss 226: 0.03322563320398331\n",
      "Loss 227: 0.03538936376571655\n",
      "Loss 228: 0.03390398621559143\n",
      "Loss 229: 0.0366177000105381\n",
      "Loss 230: 0.03548487648367882\n",
      "Loss 231: 0.03483385592699051\n",
      "Loss 232: 0.0372401662170887\n",
      "Loss 233: 0.036434538662433624\n",
      "Loss 234: 0.036827512085437775\n",
      "Loss 235: 0.03887546434998512\n",
      "Loss 236: 0.03575091436505318\n",
      "Loss 237: 0.035413384437561035\n",
      "Loss 238: 0.03628749027848244\n",
      "Loss 239: 0.03707622364163399\n",
      "Loss 240: 0.037642061710357666\n",
      "Loss 241: 0.035843152552843094\n",
      "Loss 242: 0.03651878237724304\n",
      "Loss 243: 0.037998996675014496\n",
      "Loss 244: 0.03576138615608215\n",
      "Loss 245: 0.036334287375211716\n",
      "Loss 246: 0.034644488245248795\n",
      "Loss 247: 0.03603661432862282\n",
      "Loss 248: 0.03531535342335701\n",
      "Loss 249: 0.03459008038043976\n",
      "Loss 250: 0.034261882305145264\n",
      "Loss 251: 0.03450177609920502\n",
      "Loss 252: 0.03383434936404228\n",
      "Loss 253: 0.035950180143117905\n",
      "Loss 254: 0.035408906638622284\n",
      "Loss 255: 0.03326643630862236\n",
      "Loss 256: 0.03325838968157768\n",
      "Loss 257: 0.033267226070165634\n",
      "Loss 258: 0.03235785290598869\n",
      "Loss 259: 0.03227023780345917\n",
      "Loss 260: 0.03284483775496483\n",
      "Loss 261: 0.03199765458703041\n",
      "Loss 262: 0.03150084242224693\n",
      "Loss 263: 0.03243882954120636\n",
      "Loss 264: 0.032601021230220795\n",
      "Loss 265: 0.034115418791770935\n",
      "Loss 266: 0.03172033652663231\n",
      "Loss 267: 0.031569045037031174\n",
      "Loss 268: 0.031761061400175095\n",
      "Loss 269: 0.03140965849161148\n",
      "Loss 270: 0.03283238038420677\n",
      "Loss 271: 0.0319826602935791\n",
      "Loss 272: 0.03301750868558884\n",
      "Loss 273: 0.031415700912475586\n",
      "Loss 274: 0.032630112022161484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 275: 0.0314553938806057\n",
      "Loss 276: 0.03148644044995308\n",
      "Loss 277: 0.032026126980781555\n",
      "Loss 278: 0.032578594982624054\n",
      "Loss 279: 0.03203355893492699\n",
      "Loss 280: 0.03269418329000473\n",
      "Loss 281: 0.03523539379239082\n",
      "Loss 282: 0.03651675581932068\n",
      "Loss 283: 0.03524798899888992\n",
      "Loss 284: 0.04048764333128929\n",
      "Loss 285: 0.04049910232424736\n",
      "Loss 286: 0.03888662904500961\n",
      "Loss 287: 0.03991616517305374\n",
      "Loss 288: 0.044012635946273804\n",
      "Loss 289: 0.04065961763262749\n",
      "Loss 290: 0.04178326576948166\n",
      "Loss 291: 0.048296328634023666\n",
      "Loss 292: 0.04881542548537254\n",
      "Loss 293: 0.047010134905576706\n",
      "Loss 294: 0.05053158476948738\n",
      "Loss 295: 0.051027633249759674\n",
      "Loss 296: 0.05209317058324814\n",
      "Loss 297: 0.04909228906035423\n",
      "Loss 298: 0.05111994594335556\n",
      "Loss 299: 0.05677446722984314\n",
      "Loss 300: 0.04973769187927246\n",
      "Loss 301: 0.054310400038957596\n",
      "Loss 302: 0.05336908623576164\n",
      "Loss 303: 0.054718244820833206\n",
      "Loss 304: 0.0523172989487648\n",
      "Loss 305: 0.05287706106901169\n",
      "Loss 306: 0.05122405290603638\n",
      "Loss 307: 0.0517776682972908\n",
      "Loss 308: 0.05033005774021149\n",
      "Loss 309: 0.047241318970918655\n",
      "Loss 310: 0.04752050340175629\n",
      "Loss 311: 0.04838116839528084\n",
      "Loss 312: 0.04895104095339775\n",
      "Loss 313: 0.048374831676483154\n",
      "Loss 314: 0.04799699783325195\n",
      "Loss 315: 0.04718134552240372\n",
      "Loss 316: 0.04656306281685829\n",
      "Loss 317: 0.046938858926296234\n",
      "Loss 318: 0.04532317817211151\n",
      "Loss 319: 0.04467543959617615\n",
      "Loss 320: 0.046110332012176514\n",
      "Loss 321: 0.04441167786717415\n",
      "Loss 322: 0.04348139837384224\n",
      "Loss 323: 0.04432407394051552\n",
      "Loss 324: 0.046768948435783386\n",
      "Loss 325: 0.0467982217669487\n",
      "Loss 326: 0.0441976860165596\n",
      "Loss 327: 0.0470295250415802\n",
      "Loss 328: 0.04377952218055725\n",
      "Loss 329: 0.04269589111208916\n",
      "Loss 330: 0.045023154467344284\n",
      "Loss 331: 0.0413079708814621\n",
      "Loss 332: 0.043915439397096634\n",
      "Loss 333: 0.04132811725139618\n",
      "Loss 334: 0.04381321743130684\n",
      "Loss 335: 0.04235004633665085\n",
      "Loss 336: 0.03946507349610329\n",
      "Loss 337: 0.04372552037239075\n",
      "Loss 338: 0.04024899750947952\n",
      "Loss 339: 0.040665071457624435\n",
      "Loss 340: 0.041593533009290695\n",
      "Loss 341: 0.040084730833768845\n",
      "Loss 342: 0.04076039791107178\n",
      "Loss 343: 0.03958940505981445\n",
      "Loss 344: 0.03989403694868088\n",
      "Loss 345: 0.03956092149019241\n",
      "Loss 346: 0.03894126042723656\n",
      "Loss 347: 0.03740318492054939\n",
      "Loss 348: 0.03869263082742691\n",
      "Loss 349: 0.03719072788953781\n",
      "Loss 350: 0.038245294243097305\n",
      "Loss 351: 0.037410058081150055\n",
      "Loss 352: 0.037996355444192886\n",
      "Loss 353: 0.03829055279493332\n",
      "Loss 354: 0.03687136247754097\n",
      "Loss 355: 0.03620819002389908\n",
      "Loss 356: 0.03795069083571434\n",
      "Loss 357: 0.03507399186491966\n",
      "Loss 358: 0.03908346965909004\n",
      "Loss 359: 0.03554397076368332\n",
      "Loss 360: 0.03519380837678909\n",
      "Loss 361: 0.03485621139407158\n",
      "Loss 362: 0.035107746720314026\n",
      "Loss 363: 0.034759946167469025\n",
      "Loss 364: 0.03572659194469452\n",
      "Loss 365: 0.03433377668261528\n",
      "Loss 366: 0.03360263630747795\n",
      "Loss 367: 0.03523177653551102\n",
      "Loss 368: 0.035315096378326416\n",
      "Loss 369: 0.03349659964442253\n",
      "Loss 370: 0.03380591794848442\n",
      "Loss 371: 0.03406775742769241\n",
      "Loss 372: 0.033068735152482986\n",
      "Loss 373: 0.033539775758981705\n",
      "Loss 374: 0.03410165756940842\n",
      "Loss 375: 0.03298511356115341\n",
      "Loss 376: 0.033172450959682465\n",
      "Loss 377: 0.03206169977784157\n",
      "Loss 378: 0.03424873948097229\n",
      "Loss 379: 0.03434794396162033\n",
      "Loss 380: 0.033854808658361435\n",
      "Loss 381: 0.03404977172613144\n",
      "Loss 382: 0.032081276178359985\n",
      "Loss 383: 0.0328252837061882\n",
      "Loss 384: 0.03294831141829491\n",
      "Loss 385: 0.03455587103962898\n",
      "Loss 386: 0.03274135664105415\n",
      "Loss 387: 0.03230903297662735\n",
      "Loss 388: 0.03245842456817627\n",
      "Loss 389: 0.03185444325208664\n",
      "Loss 390: 0.03286629170179367\n",
      "Loss 391: 0.03173702210187912\n",
      "Loss 392: 0.03212881460785866\n",
      "Loss 393: 0.031899675726890564\n",
      "Loss 394: 0.03183290734887123\n",
      "Loss 395: 0.030455881729722023\n",
      "Loss 396: 0.0313057079911232\n",
      "Loss 397: 0.03192002326250076\n",
      "Loss 398: 0.033454928547143936\n",
      "Loss 399: 0.03127520531415939\n",
      "Loss 400: 0.03211117535829544\n",
      "Loss 401: 0.033394359052181244\n",
      "Loss 402: 0.03406789153814316\n",
      "Loss 403: 0.036796048283576965\n",
      "Loss 404: 0.03499247133731842\n",
      "Loss 405: 0.03520883992314339\n",
      "Loss 406: 0.036457259207963943\n",
      "Loss 407: 0.03673165664076805\n",
      "Loss 408: 0.033781372010707855\n",
      "Loss 409: 0.03838614001870155\n",
      "Loss 410: 0.038966238498687744\n",
      "Loss 411: 0.035546258091926575\n",
      "Loss 412: 0.036584414541721344\n",
      "Loss 413: 0.03637569397687912\n",
      "Loss 414: 0.03580299764871597\n",
      "Loss 415: 0.03552868217229843\n",
      "Loss 416: 0.035647179931402206\n",
      "Loss 417: 0.037200678139925\n",
      "Loss 418: 0.03561875969171524\n",
      "Loss 419: 0.034796092659235\n",
      "Loss 420: 0.035185374319553375\n",
      "Loss 421: 0.03632984682917595\n",
      "Loss 422: 0.037143606692552567\n",
      "Loss 423: 0.036823734641075134\n",
      "Loss 424: 0.036633510142564774\n",
      "Loss 425: 0.03492092713713646\n",
      "Loss 426: 0.035385873168706894\n",
      "Loss 427: 0.03605665639042854\n",
      "Loss 428: 0.035869479179382324\n",
      "Loss 429: 0.0367165207862854\n",
      "Loss 430: 0.0333557091653347\n",
      "Loss 431: 0.03462408855557442\n",
      "Loss 432: 0.036240555346012115\n",
      "Loss 433: 0.03417086973786354\n",
      "Loss 434: 0.03380848839879036\n",
      "Loss 435: 0.03402557969093323\n",
      "Loss 436: 0.03399785980582237\n",
      "Loss 437: 0.03479689359664917\n",
      "Loss 438: 0.03341752290725708\n",
      "Loss 439: 0.03466635197401047\n",
      "Loss 440: 0.032881397753953934\n",
      "Loss 441: 0.03323706239461899\n",
      "Loss 442: 0.03430807590484619\n",
      "Loss 443: 0.032983437180519104\n",
      "Loss 444: 0.033371202647686005\n",
      "Loss 445: 0.033629171550273895\n",
      "Loss 446: 0.03353112190961838\n",
      "Loss 447: 0.0329863503575325\n",
      "Loss 448: 0.03446170687675476\n",
      "Loss 449: 0.033500950783491135\n",
      "Loss 450: 0.032296109944581985\n",
      "Loss 451: 0.033127687871456146\n",
      "Loss 452: 0.03399777412414551\n",
      "Loss 453: 0.03302059322595596\n",
      "Loss 454: 0.032718926668167114\n",
      "Loss 455: 0.03402255102992058\n",
      "Loss 456: 0.032522883266210556\n",
      "Loss 457: 0.033124130219221115\n",
      "Loss 458: 0.03237792104482651\n",
      "Loss 459: 0.03363893926143646\n",
      "Loss 460: 0.03278542682528496\n",
      "Loss 461: 0.03203946724534035\n",
      "Loss 462: 0.03249134495854378\n",
      "Loss 463: 0.03345013037323952\n",
      "Loss 464: 0.032737910747528076\n",
      "Loss 465: 0.03158758208155632\n",
      "Loss 466: 0.031436383724212646\n",
      "Loss 467: 0.033329546451568604\n",
      "Loss 468: 0.0303878765553236\n",
      "Loss 469: 0.030739735811948776\n",
      "Loss 470: 0.032196491956710815\n",
      "Loss 471: 0.03245021030306816\n",
      "Loss 472: 0.031456269323825836\n",
      "Loss 473: 0.030836353078484535\n",
      "Loss 474: 0.03130719065666199\n",
      "Loss 475: 0.031583696603775024\n",
      "Loss 476: 0.030145060271024704\n",
      "Loss 477: 0.031344495713710785\n",
      "Loss 478: 0.03042643703520298\n",
      "Loss 479: 0.029725920408964157\n",
      "Loss 480: 0.030569976195693016\n",
      "Loss 481: 0.029126768931746483\n",
      "Loss 482: 0.028894146904349327\n",
      "Loss 483: 0.030215896666049957\n",
      "Loss 484: 0.030979864299297333\n",
      "Loss 485: 0.030940251424908638\n",
      "Loss 486: 0.03109649010002613\n",
      "Loss 487: 0.02945411205291748\n",
      "Loss 488: 0.029656926169991493\n",
      "Loss 489: 0.030001657083630562\n",
      "Loss 490: 0.03044821321964264\n",
      "Loss 491: 0.029690388590097427\n",
      "Loss 492: 0.02946309559047222\n",
      "Loss 493: 0.02987881377339363\n",
      "Loss 494: 0.029917962849140167\n",
      "Loss 495: 0.03033638186752796\n",
      "Loss 496: 0.030098313465714455\n",
      "Loss 497: 0.02967628464102745\n",
      "Loss 498: 0.03008262626826763\n",
      "Loss 499: 0.029166875407099724\n",
      "Loss 500: 0.029942404478788376\n",
      "Loss 501: 0.029823962599039078\n",
      "Loss 502: 0.029759271070361137\n",
      "Loss 503: 0.029974710196256638\n",
      "Loss 504: 0.03157301992177963\n",
      "Loss 505: 0.03034628927707672\n",
      "Loss 506: 0.032111912965774536\n",
      "Loss 507: 0.030610065907239914\n",
      "Loss 508: 0.03048117831349373\n",
      "Loss 509: 0.030849123373627663\n",
      "Loss 510: 0.03179784491658211\n",
      "Loss 511: 0.031656112521886826\n",
      "Loss 512: 0.03292939439415932\n",
      "Loss 513: 0.03227265179157257\n",
      "Loss 514: 0.030864030122756958\n",
      "Loss 515: 0.031237706542015076\n",
      "Loss 516: 0.03091668151319027\n",
      "Loss 517: 0.03265184536576271\n",
      "Loss 518: 0.03196407109498978\n",
      "Loss 519: 0.03198225423693657\n",
      "Loss 520: 0.03275587409734726\n",
      "Loss 521: 0.03209606185555458\n",
      "Loss 522: 0.03277331218123436\n",
      "Loss 523: 0.032653361558914185\n",
      "Loss 524: 0.033439379185438156\n",
      "Loss 525: 0.0321807898581028\n",
      "Loss 526: 0.03236326947808266\n",
      "Loss 527: 0.03324338048696518\n",
      "Loss 528: 0.03407839313149452\n",
      "Loss 529: 0.03442056104540825\n",
      "Loss 530: 0.033105190843343735\n",
      "Loss 531: 0.03462357074022293\n",
      "Loss 532: 0.035509318113327026\n",
      "Loss 533: 0.03816952556371689\n",
      "Loss 534: 0.03913550078868866\n",
      "Loss 535: 0.041888315230607986\n",
      "Loss 536: 0.039577666670084\n",
      "Loss 537: 0.04360331594944\n",
      "Loss 538: 0.04482589289546013\n",
      "Loss 539: 0.04139396548271179\n",
      "Loss 540: 0.045979034155607224\n",
      "Loss 541: 0.04133002460002899\n",
      "Loss 542: 0.04576944187283516\n",
      "Loss 543: 0.04356904327869415\n",
      "Loss 544: 0.04302087426185608\n",
      "Loss 545: 0.04153573140501976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 546: 0.04332321882247925\n",
      "Loss 547: 0.044246938079595566\n",
      "Loss 548: 0.04180968180298805\n",
      "Loss 549: 0.043997954577207565\n",
      "Loss 550: 0.041633784770965576\n",
      "Loss 551: 0.04316529631614685\n",
      "Loss 552: 0.041501887142658234\n",
      "Loss 553: 0.04056589677929878\n",
      "Loss 554: 0.043218888342380524\n",
      "Loss 555: 0.04166660085320473\n",
      "Loss 556: 0.04307598993182182\n",
      "Loss 557: 0.04171241074800491\n",
      "Loss 558: 0.041200608015060425\n",
      "Loss 559: 0.039146389812231064\n",
      "Loss 560: 0.03934953361749649\n",
      "Loss 561: 0.04085272550582886\n",
      "Loss 562: 0.03930840641260147\n",
      "Loss 563: 0.04064682498574257\n",
      "Loss 564: 0.03906990587711334\n",
      "Loss 565: 0.03942374512553215\n",
      "Loss 566: 0.03719382360577583\n",
      "Loss 567: 0.03861053287982941\n",
      "Loss 568: 0.04087095335125923\n",
      "Loss 569: 0.03763872757554054\n",
      "Loss 570: 0.03837102651596069\n",
      "Loss 571: 0.03675369545817375\n",
      "Loss 572: 0.03629233315587044\n",
      "Loss 573: 0.035867199301719666\n",
      "Loss 574: 0.037433795630931854\n",
      "Loss 575: 0.03765552490949631\n",
      "Loss 576: 0.03489591181278229\n",
      "Loss 577: 0.03748803585767746\n",
      "Loss 578: 0.03684016317129135\n",
      "Loss 579: 0.03673005476593971\n",
      "Loss 580: 0.03758712485432625\n",
      "Loss 581: 0.03621474280953407\n",
      "Loss 582: 0.03660359978675842\n",
      "Loss 583: 0.0370471253991127\n",
      "Loss 584: 0.035777006298303604\n",
      "Loss 585: 0.03561801090836525\n",
      "Loss 586: 0.03422435373067856\n",
      "Loss 587: 0.03549234941601753\n",
      "Loss 588: 0.03487096726894379\n",
      "Loss 589: 0.03622277453541756\n",
      "Loss 590: 0.0334988497197628\n",
      "Loss 591: 0.03374865651130676\n",
      "Loss 592: 0.03443446755409241\n",
      "Loss 593: 0.03445597365498543\n",
      "Loss 594: 0.03345993161201477\n",
      "Loss 595: 0.03398151323199272\n",
      "Loss 596: 0.03435900807380676\n",
      "Loss 597: 0.033472828567028046\n",
      "Loss 598: 0.03327680751681328\n",
      "Loss 599: 0.032660335302352905\n",
      "Loss 600: 0.032430499792099\n",
      "Loss 601: 0.033120185136795044\n",
      "Loss 602: 0.03390161693096161\n",
      "Loss 603: 0.033395539969205856\n",
      "Loss 604: 0.032435789704322815\n",
      "Loss 605: 0.03313535079360008\n",
      "Loss 606: 0.03438074141740799\n",
      "Loss 607: 0.03285596892237663\n",
      "Loss 608: 0.03255237638950348\n",
      "Loss 609: 0.030926942825317383\n",
      "Loss 610: 0.03223979473114014\n",
      "Loss 611: 0.03134859353303909\n",
      "Loss 612: 0.03179309517145157\n",
      "Loss 613: 0.030534129589796066\n",
      "Loss 614: 0.031064635142683983\n",
      "Loss 615: 0.03196607530117035\n",
      "Loss 616: 0.032149527221918106\n",
      "Loss 617: 0.032706696540117264\n",
      "Loss 618: 0.03136088326573372\n",
      "Loss 619: 0.03034687601029873\n",
      "Loss 620: 0.03139697387814522\n",
      "Loss 621: 0.02976638823747635\n",
      "Loss 622: 0.03164118528366089\n",
      "Loss 623: 0.03066951036453247\n",
      "Loss 624: 0.0311417356133461\n",
      "Loss 625: 0.031746502965688705\n",
      "Loss 626: 0.03185555711388588\n",
      "Loss 627: 0.03155083209276199\n",
      "Loss 628: 0.0315755233168602\n",
      "Loss 629: 0.029954908415675163\n",
      "Loss 630: 0.031022775918245316\n",
      "Loss 631: 0.031294889748096466\n",
      "Loss 632: 0.03218691423535347\n",
      "Loss 633: 0.03156006708741188\n",
      "Loss 634: 0.030489100143313408\n",
      "Loss 635: 0.03053191676735878\n",
      "Loss 636: 0.030667012557387352\n",
      "Loss 637: 0.030733799561858177\n",
      "Loss 638: 0.03594377264380455\n",
      "Loss 639: 0.03126126527786255\n",
      "Loss 640: 0.0338112972676754\n",
      "Loss 641: 0.03874757140874863\n",
      "Loss 642: 0.035874564200639725\n",
      "Loss 643: 0.037339940667152405\n",
      "Loss 644: 0.03833450749516487\n",
      "Loss 645: 0.03948798030614853\n",
      "Loss 646: 0.03834043815732002\n",
      "Loss 647: 0.039151035249233246\n",
      "Loss 648: 0.03852425888180733\n",
      "Loss 649: 0.039213575422763824\n",
      "Loss 650: 0.03836173191666603\n",
      "Loss 651: 0.037921562790870667\n",
      "Loss 652: 0.04136280715465546\n",
      "Loss 653: 0.03955243527889252\n",
      "Loss 654: 0.039886921644210815\n",
      "Loss 655: 0.03942850977182388\n",
      "Loss 656: 0.039228327572345734\n",
      "Loss 657: 0.03902913257479668\n",
      "Loss 658: 0.03918476402759552\n",
      "Loss 659: 0.0381753034889698\n",
      "Loss 660: 0.03870779275894165\n",
      "Loss 661: 0.03832239657640457\n",
      "Loss 662: 0.038285523653030396\n",
      "Loss 663: 0.037846751511096954\n",
      "Loss 664: 0.03758900985121727\n",
      "Loss 665: 0.0373854897916317\n",
      "Loss 666: 0.03765188530087471\n",
      "Loss 667: 0.03785292059183121\n",
      "Loss 668: 0.037454478442668915\n",
      "Loss 669: 0.03616799786686897\n",
      "Loss 670: 0.03716340288519859\n",
      "Loss 671: 0.03675553947687149\n",
      "Loss 672: 0.03750549629330635\n",
      "Loss 673: 0.03614494204521179\n",
      "Loss 674: 0.03453195467591286\n",
      "Loss 675: 0.03561297431588173\n",
      "Loss 676: 0.03570196032524109\n",
      "Loss 677: 0.034149013459682465\n",
      "Loss 678: 0.034143105149269104\n",
      "Loss 679: 0.03506728261709213\n",
      "Loss 680: 0.03403497859835625\n",
      "Loss 681: 0.03533688932657242\n",
      "Loss 682: 0.033612463623285294\n",
      "Loss 683: 0.03426653519272804\n",
      "Loss 684: 0.03361994028091431\n",
      "Loss 685: 0.033126141875982285\n",
      "Loss 686: 0.034737274050712585\n",
      "Loss 687: 0.0320926196873188\n",
      "Loss 688: 0.032813530415296555\n",
      "Loss 689: 0.03269011154770851\n",
      "Loss 690: 0.032616566866636276\n",
      "Loss 691: 0.03399737924337387\n",
      "Loss 692: 0.03410917893052101\n",
      "Loss 693: 0.033631306141614914\n",
      "Loss 694: 0.03278126195073128\n",
      "Loss 695: 0.03415759652853012\n",
      "Loss 696: 0.030973609536886215\n",
      "Loss 697: 0.03296096250414848\n",
      "Loss 698: 0.03228439763188362\n",
      "Loss 699: 0.03120134398341179\n",
      "Loss 700: 0.030866064131259918\n",
      "Loss 701: 0.03215014934539795\n",
      "Loss 702: 0.03143865242600441\n",
      "Loss 703: 0.031218890100717545\n",
      "Loss 704: 0.03108079545199871\n",
      "Loss 705: 0.03174616023898125\n",
      "Loss 706: 0.03167639672756195\n",
      "Loss 707: 0.030529392883181572\n",
      "Loss 708: 0.03017204999923706\n",
      "Loss 709: 0.030317341908812523\n",
      "Loss 710: 0.030863456428050995\n",
      "Loss 711: 0.031109431758522987\n",
      "Loss 712: 0.030269887298345566\n",
      "Loss 713: 0.02961495891213417\n",
      "Loss 714: 0.029225829988718033\n",
      "Loss 715: 0.03236832842230797\n",
      "Loss 716: 0.03145870193839073\n",
      "Loss 717: 0.030197424814105034\n",
      "Loss 718: 0.030019229277968407\n",
      "Loss 719: 0.029754219576716423\n",
      "Loss 720: 0.031510889530181885\n",
      "Loss 721: 0.029587505385279655\n",
      "Loss 722: 0.030722204595804214\n",
      "Loss 723: 0.030018296092748642\n",
      "Loss 724: 0.029521137475967407\n",
      "Loss 725: 0.029188185930252075\n",
      "Loss 726: 0.029664350673556328\n",
      "Loss 727: 0.03015839494764805\n",
      "Loss 728: 0.0305084940046072\n",
      "Loss 729: 0.028962040320038795\n",
      "Loss 730: 0.028743388131260872\n",
      "Loss 731: 0.030187824741005898\n",
      "Loss 732: 0.029909467324614525\n",
      "Loss 733: 0.030864477157592773\n",
      "Loss 734: 0.028985748067498207\n",
      "Loss 735: 0.030856039375066757\n",
      "Loss 736: 0.03004593774676323\n",
      "Loss 737: 0.03061755932867527\n",
      "Loss 738: 0.03060358762741089\n",
      "Loss 739: 0.029528701677918434\n",
      "Loss 740: 0.031958747655153275\n",
      "Loss 741: 0.03311103209853172\n",
      "Loss 742: 0.031810060143470764\n",
      "Loss 743: 0.033173978328704834\n",
      "Loss 744: 0.037633802741765976\n",
      "Loss 745: 0.036888424307107925\n",
      "Loss 746: 0.0431676022708416\n",
      "Loss 747: 0.043543387204408646\n",
      "Loss 748: 0.042671579867601395\n",
      "Loss 749: 0.04380173608660698\n",
      "Loss 750: 0.04411786422133446\n",
      "Loss 751: 0.04585227742791176\n",
      "Loss 752: 0.04282205179333687\n",
      "Loss 753: 0.048448726534843445\n",
      "Loss 754: 0.04712044075131416\n",
      "Loss 755: 0.04477333277463913\n",
      "Loss 756: 0.045992158353328705\n",
      "Loss 757: 0.046943482011556625\n",
      "Loss 758: 0.04484202712774277\n",
      "Loss 759: 0.045787468552589417\n",
      "Loss 760: 0.04713121056556702\n",
      "Loss 761: 0.04497124254703522\n",
      "Loss 762: 0.04248887300491333\n",
      "Loss 763: 0.043011415749788284\n",
      "Loss 764: 0.04468193277716637\n",
      "Loss 765: 0.04304669052362442\n",
      "Loss 766: 0.04123084992170334\n",
      "Loss 767: 0.04283080995082855\n",
      "Loss 768: 0.04393968731164932\n",
      "Loss 769: 0.04310882091522217\n",
      "Loss 770: 0.04007001966238022\n",
      "Loss 771: 0.041133202612400055\n",
      "Loss 772: 0.04102195054292679\n",
      "Loss 773: 0.04104139655828476\n",
      "Loss 774: 0.042293645441532135\n",
      "Loss 775: 0.04104403778910637\n",
      "Loss 776: 0.04165071249008179\n",
      "Loss 777: 0.04163932427763939\n",
      "Loss 778: 0.04049459844827652\n",
      "Loss 779: 0.04042033851146698\n",
      "Loss 780: 0.03926362469792366\n",
      "Loss 781: 0.03923559933900833\n",
      "Loss 782: 0.03940972685813904\n",
      "Loss 783: 0.03963470458984375\n",
      "Loss 784: 0.03857899829745293\n",
      "Loss 785: 0.03827299550175667\n",
      "Loss 786: 0.03905029222369194\n",
      "Loss 787: 0.039668381214141846\n",
      "Loss 788: 0.03613020107150078\n",
      "Loss 789: 0.03865877166390419\n",
      "Loss 790: 0.037442971020936966\n",
      "Loss 791: 0.03718545660376549\n",
      "Loss 792: 0.03795261308550835\n",
      "Loss 793: 0.035953838378190994\n",
      "Loss 794: 0.036581285297870636\n",
      "Loss 795: 0.03715848922729492\n",
      "Loss 796: 0.03612678125500679\n",
      "Loss 797: 0.03507981821894646\n",
      "Loss 798: 0.03679370507597923\n",
      "Loss 799: 0.034453634172677994\n",
      "Loss 800: 0.034313831478357315\n",
      "Loss 801: 0.03416174277663231\n",
      "Loss 802: 0.03686676174402237\n",
      "Loss 803: 0.03622005879878998\n",
      "Loss 804: 0.034993864595890045\n",
      "Loss 805: 0.03487582504749298\n",
      "Loss 806: 0.03548760712146759\n",
      "Loss 807: 0.03449930250644684\n",
      "Loss 808: 0.036348436027765274\n",
      "Loss 809: 0.03271996229887009\n",
      "Loss 810: 0.03396740183234215\n",
      "Loss 811: 0.03334079682826996\n",
      "Loss 812: 0.0350567065179348\n",
      "Loss 813: 0.033208172768354416\n",
      "Loss 814: 0.03365936875343323\n",
      "Loss 815: 0.03326840698719025\n",
      "Loss 816: 0.03266626596450806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 817: 0.03295695409178734\n",
      "Loss 818: 0.0324990376830101\n",
      "Loss 819: 0.03266531229019165\n",
      "Loss 820: 0.03329260274767876\n",
      "Loss 821: 0.032293353229761124\n",
      "Loss 822: 0.03148063272237778\n",
      "Loss 823: 0.033144522458314896\n",
      "Loss 824: 0.03273024037480354\n",
      "Loss 825: 0.031269680708646774\n",
      "Loss 826: 0.0313858687877655\n",
      "Loss 827: 0.032464705407619476\n",
      "Loss 828: 0.030563142150640488\n",
      "Loss 829: 0.030923936516046524\n",
      "Loss 830: 0.03064611554145813\n",
      "Loss 831: 0.03156512603163719\n",
      "Loss 832: 0.0313887894153595\n",
      "Loss 833: 0.030849458649754524\n",
      "Loss 834: 0.030762003734707832\n",
      "Loss 835: 0.030909111723303795\n",
      "Loss 836: 0.030257156118750572\n",
      "Loss 837: 0.03179437294602394\n",
      "Loss 838: 0.03112446330487728\n",
      "Loss 839: 0.029791807755827904\n",
      "Loss 840: 0.029405182227492332\n",
      "Loss 841: 0.03040340729057789\n",
      "Loss 842: 0.0284589696675539\n",
      "Loss 843: 0.029551127925515175\n",
      "Loss 844: 0.030891621485352516\n",
      "Loss 845: 0.03018355369567871\n",
      "Loss 846: 0.03144265338778496\n",
      "Loss 847: 0.030188430100679398\n",
      "Loss 848: 0.027691874653100967\n",
      "Loss 849: 0.03062313050031662\n",
      "Loss 850: 0.03084304742515087\n",
      "Loss 851: 0.03158838674426079\n",
      "Loss 852: 0.031467609107494354\n",
      "Loss 853: 0.03287250176072121\n",
      "Loss 854: 0.032069090753793716\n",
      "Loss 855: 0.034740231931209564\n",
      "Loss 856: 0.034884143620729446\n",
      "Loss 857: 0.03489329665899277\n",
      "Loss 858: 0.03504263982176781\n",
      "Loss 859: 0.037397656589746475\n",
      "Loss 860: 0.03631629794836044\n",
      "Loss 861: 0.04094063490629196\n",
      "Loss 862: 0.039333950728178024\n",
      "Loss 863: 0.04074275493621826\n",
      "Loss 864: 0.03868865221738815\n",
      "Loss 865: 0.03875471651554108\n",
      "Loss 866: 0.041612595319747925\n",
      "Loss 867: 0.041647519916296005\n",
      "Loss 868: 0.03933228552341461\n",
      "Loss 869: 0.040991298854351044\n",
      "Loss 870: 0.039871737360954285\n",
      "Loss 871: 0.03984827548265457\n",
      "Loss 872: 0.040299173444509506\n",
      "Loss 873: 0.03889450803399086\n",
      "Loss 874: 0.038664381951093674\n",
      "Loss 875: 0.038929782807826996\n",
      "Loss 876: 0.03909621387720108\n",
      "Loss 877: 0.03844886273145676\n",
      "Loss 878: 0.03738155961036682\n",
      "Loss 879: 0.03751559928059578\n",
      "Loss 880: 0.038401201367378235\n",
      "Loss 881: 0.03743242472410202\n",
      "Loss 882: 0.0384671725332737\n",
      "Loss 883: 0.038525599986314774\n",
      "Loss 884: 0.03765762597322464\n",
      "Loss 885: 0.03634055703878403\n",
      "Loss 886: 0.038283150643110275\n",
      "Loss 887: 0.036593224853277206\n",
      "Loss 888: 0.03652738779783249\n",
      "Loss 889: 0.037821684032678604\n",
      "Loss 890: 0.036930590867996216\n",
      "Loss 891: 0.03581031784415245\n",
      "Loss 892: 0.03571038320660591\n",
      "Loss 893: 0.036032259464263916\n",
      "Loss 894: 0.0342455692589283\n",
      "Loss 895: 0.035029489547014236\n",
      "Loss 896: 0.033923350274562836\n",
      "Loss 897: 0.034649353474378586\n",
      "Loss 898: 0.03460909053683281\n",
      "Loss 899: 0.03425963968038559\n",
      "Loss 900: 0.034898433834314346\n",
      "Loss 901: 0.03556528687477112\n",
      "Loss 902: 0.03266878426074982\n",
      "Loss 903: 0.034214768558740616\n",
      "Loss 904: 0.03491782024502754\n",
      "Loss 905: 0.03444339707493782\n",
      "Loss 906: 0.03368796780705452\n",
      "Loss 907: 0.033935531973838806\n",
      "Loss 908: 0.03326626867055893\n",
      "Loss 909: 0.033696357160806656\n",
      "Loss 910: 0.03242546319961548\n",
      "Loss 911: 0.03248991444706917\n",
      "Loss 912: 0.03320176154375076\n",
      "Loss 913: 0.03320935368537903\n",
      "Loss 914: 0.03270929306745529\n",
      "Loss 915: 0.033408645540475845\n",
      "Loss 916: 0.0322425402700901\n",
      "Loss 917: 0.031832270324230194\n",
      "Loss 918: 0.03223410248756409\n",
      "Loss 919: 0.032506201416254044\n",
      "Loss 920: 0.032460231333971024\n",
      "Loss 921: 0.03332339599728584\n",
      "Loss 922: 0.03206067904829979\n",
      "Loss 923: 0.03270186856389046\n",
      "Loss 924: 0.03166912496089935\n",
      "Loss 925: 0.03443203866481781\n",
      "Loss 926: 0.03349468484520912\n",
      "Loss 927: 0.03539661690592766\n",
      "Loss 928: 0.03826630115509033\n",
      "Loss 929: 0.037113092839717865\n",
      "Loss 930: 0.037987738847732544\n",
      "Loss 931: 0.038346774876117706\n",
      "Loss 932: 0.03889736533164978\n",
      "Loss 933: 0.04026367887854576\n",
      "Loss 934: 0.040235407650470734\n",
      "Loss 935: 0.0383247472345829\n",
      "Loss 936: 0.038859717547893524\n",
      "Loss 937: 0.040310248732566833\n",
      "Loss 938: 0.03888129070401192\n",
      "Loss 939: 0.042815420776605606\n",
      "Loss 940: 0.04022473096847534\n",
      "Loss 941: 0.037650708109140396\n",
      "Loss 942: 0.0398719348013401\n",
      "Loss 943: 0.03940126299858093\n",
      "Loss 944: 0.037108056247234344\n",
      "Loss 945: 0.038520488888025284\n",
      "Loss 946: 0.036850716918706894\n",
      "Loss 947: 0.03908200189471245\n",
      "Loss 948: 0.03735312074422836\n",
      "Loss 949: 0.03835009038448334\n",
      "Loss 950: 0.03746720030903816\n",
      "Loss 951: 0.03885819390416145\n",
      "Loss 952: 0.037307728081941605\n",
      "Loss 953: 0.03827526047825813\n",
      "Loss 954: 0.03538457676768303\n",
      "Loss 955: 0.03678308427333832\n",
      "Loss 956: 0.035470664501190186\n",
      "Loss 957: 0.03721611574292183\n",
      "Loss 958: 0.03539222851395607\n",
      "Loss 959: 0.036350078880786896\n",
      "Loss 960: 0.034882500767707825\n",
      "Loss 961: 0.035966742783784866\n",
      "Loss 962: 0.03543046489357948\n",
      "Loss 963: 0.03558002784848213\n",
      "Loss 964: 0.0348593108355999\n",
      "Loss 965: 0.03351682052016258\n",
      "Loss 966: 0.03549909219145775\n",
      "Loss 967: 0.033709850162267685\n",
      "Loss 968: 0.03416561707854271\n",
      "Loss 969: 0.033827342092990875\n",
      "Loss 970: 0.03377293050289154\n",
      "Loss 971: 0.03390884771943092\n",
      "Loss 972: 0.03574845567345619\n",
      "Loss 973: 0.0334259569644928\n",
      "Loss 974: 0.03352276235818863\n",
      "Loss 975: 0.032792918384075165\n",
      "Loss 976: 0.03445422649383545\n",
      "Loss 977: 0.03443511202931404\n",
      "Loss 978: 0.03259136900305748\n",
      "Loss 979: 0.032324355095624924\n",
      "Loss 980: 0.03381185978651047\n",
      "Loss 981: 0.03321431949734688\n",
      "Loss 982: 0.031448446214199066\n",
      "Loss 983: 0.03253708779811859\n",
      "Loss 984: 0.033160533756017685\n",
      "Loss 985: 0.03136977553367615\n",
      "Loss 986: 0.032622259110212326\n",
      "Loss 987: 0.03405971825122833\n",
      "Loss 988: 0.032026100903749466\n",
      "Loss 989: 0.03270253911614418\n",
      "Loss 990: 0.03212703391909599\n",
      "Loss 991: 0.03157767653465271\n",
      "Loss 992: 0.031774792820215225\n",
      "Loss 993: 0.031856469810009\n",
      "Loss 994: 0.03221437707543373\n",
      "Loss 995: 0.03148658946156502\n",
      "Loss 996: 0.032871030271053314\n",
      "Loss 997: 0.032688822597265244\n",
      "Loss 998: 0.033078428357839584\n",
      "Loss 999: 0.031055258587002754\n",
      "Loss 1000: 0.031504590064287186\n",
      "Loss 1001: 0.03169054910540581\n",
      "Loss 1002: 0.0316481813788414\n",
      "Loss 1003: 0.03089786134660244\n",
      "Loss 1004: 0.031003732234239578\n",
      "Loss 1005: 0.029797140508890152\n",
      "Loss 1006: 0.032182082533836365\n",
      "Loss 1007: 0.032269444316625595\n",
      "Loss 1008: 0.030749386176466942\n",
      "Loss 1009: 0.03107353486120701\n",
      "Loss 1010: 0.031232373788952827\n",
      "Loss 1011: 0.03153645619750023\n",
      "Loss 1012: 0.02969738095998764\n",
      "Loss 1013: 0.03150545805692673\n",
      "Loss 1014: 0.031687457114458084\n",
      "Loss 1015: 0.031533755362033844\n",
      "Loss 1016: 0.031161844730377197\n",
      "Loss 1017: 0.03125797212123871\n",
      "Loss 1018: 0.03076024539768696\n",
      "Loss 1019: 0.030404862016439438\n",
      "Loss 1020: 0.031353674829006195\n",
      "Loss 1021: 0.031141428276896477\n",
      "Loss 1022: 0.030693508684635162\n",
      "Loss 1023: 0.029885290190577507\n",
      "Loss 1024: 0.030475733801722527\n",
      "Loss 1025: 0.031170431524515152\n",
      "Loss 1026: 0.030268030241131783\n",
      "Loss 1027: 0.030620159581303596\n",
      "Loss 1028: 0.03047391027212143\n",
      "Loss 1029: 0.031241364777088165\n",
      "Loss 1030: 0.031373001635074615\n",
      "Loss 1031: 0.03019331395626068\n",
      "Loss 1032: 0.03154103457927704\n",
      "Loss 1033: 0.029979515820741653\n",
      "Loss 1034: 0.03138820081949234\n",
      "Loss 1035: 0.030555127188563347\n",
      "Loss 1036: 0.03061608411371708\n",
      "Loss 1037: 0.03077433630824089\n",
      "Loss 1038: 0.030393753200769424\n",
      "Loss 1039: 0.0314735509455204\n",
      "Loss 1040: 0.03092852421104908\n",
      "Loss 1041: 0.031183427199721336\n",
      "Loss 1042: 0.031063273549079895\n",
      "Loss 1043: 0.02978924848139286\n",
      "Loss 1044: 0.02985888347029686\n",
      "Loss 1045: 0.03211183845996857\n",
      "Loss 1046: 0.030087048187851906\n",
      "Loss 1047: 0.03286853805184364\n",
      "Loss 1048: 0.03208203241229057\n",
      "Loss 1049: 0.03195325285196304\n",
      "Loss 1050: 0.03362540155649185\n",
      "Loss 1051: 0.03133651241660118\n",
      "Loss 1052: 0.03148643672466278\n",
      "Loss 1053: 0.03203608840703964\n",
      "Loss 1054: 0.03213100880384445\n",
      "Loss 1055: 0.03257301449775696\n",
      "Loss 1056: 0.031916920095682144\n",
      "Loss 1057: 0.03157547861337662\n",
      "Loss 1058: 0.030845684930682182\n",
      "Loss 1059: 0.03240504115819931\n",
      "Loss 1060: 0.030126359313726425\n",
      "Loss 1061: 0.03125931695103645\n",
      "Loss 1062: 0.03207363933324814\n",
      "Loss 1063: 0.033729150891304016\n",
      "Loss 1064: 0.03179136663675308\n",
      "Loss 1065: 0.032717615365982056\n",
      "Loss 1066: 0.03380290046334267\n",
      "Loss 1067: 0.03353402763605118\n",
      "Loss 1068: 0.03378450870513916\n",
      "Loss 1069: 0.03554427996277809\n",
      "Loss 1070: 0.03618893027305603\n",
      "Loss 1071: 0.03607970476150513\n",
      "Loss 1072: 0.03592223301529884\n",
      "Loss 1073: 0.03758356347680092\n",
      "Loss 1074: 0.03594654053449631\n",
      "Loss 1075: 0.03571765124797821\n",
      "Loss 1076: 0.03573333099484444\n",
      "Loss 1077: 0.037071846425533295\n",
      "Loss 1078: 0.03537507727742195\n",
      "Loss 1079: 0.03746388480067253\n",
      "Loss 1080: 0.036697596311569214\n",
      "Loss 1081: 0.03588811680674553\n",
      "Loss 1082: 0.036775507032871246\n",
      "Loss 1083: 0.03763815015554428\n",
      "Loss 1084: 0.037201233208179474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1085: 0.03654731437563896\n",
      "Loss 1086: 0.03861230984330177\n",
      "Loss 1087: 0.036188237369060516\n",
      "Loss 1088: 0.03716781735420227\n",
      "Loss 1089: 0.0358431302011013\n",
      "Loss 1090: 0.036155492067337036\n",
      "Loss 1091: 0.03693477436900139\n",
      "Loss 1092: 0.033599164336919785\n",
      "Loss 1093: 0.03648494556546211\n",
      "Loss 1094: 0.03545602783560753\n",
      "Loss 1095: 0.03487064689397812\n",
      "Loss 1096: 0.03483317047357559\n",
      "Loss 1097: 0.03315195441246033\n",
      "Loss 1098: 0.035436708480119705\n",
      "Loss 1099: 0.03355559706687927\n",
      "Loss 1100: 0.03520527109503746\n",
      "Loss 1101: 0.034322626888751984\n",
      "Loss 1102: 0.033604543656110764\n",
      "Loss 1103: 0.033921848982572556\n",
      "Loss 1104: 0.03404891863465309\n",
      "Loss 1105: 0.033504024147987366\n",
      "Loss 1106: 0.0326552540063858\n",
      "Loss 1107: 0.0328650176525116\n",
      "Loss 1108: 0.03260482847690582\n",
      "Loss 1109: 0.03274046629667282\n",
      "Loss 1110: 0.03338818997144699\n",
      "Loss 1111: 0.03251403570175171\n",
      "Loss 1112: 0.03395140543580055\n",
      "Loss 1113: 0.033357806503772736\n",
      "Loss 1114: 0.03203481435775757\n",
      "Loss 1115: 0.031645528972148895\n",
      "Loss 1116: 0.03162940591573715\n",
      "Loss 1117: 0.033104296773672104\n",
      "Loss 1118: 0.032707907259464264\n",
      "Loss 1119: 0.032610852271318436\n",
      "Loss 1120: 0.03281090408563614\n",
      "Loss 1121: 0.03187355771660805\n",
      "Loss 1122: 0.032087210565805435\n",
      "Loss 1123: 0.03240666538476944\n",
      "Loss 1124: 0.03187892585992813\n",
      "Loss 1125: 0.03311695531010628\n",
      "Loss 1126: 0.031084630638360977\n",
      "Loss 1127: 0.03127110376954079\n",
      "Loss 1128: 0.03033040091395378\n",
      "Loss 1129: 0.031391557306051254\n",
      "Loss 1130: 0.03129730373620987\n",
      "Loss 1131: 0.03114846721291542\n",
      "Loss 1132: 0.03115815669298172\n",
      "Loss 1133: 0.03064582124352455\n",
      "Loss 1134: 0.030639275908470154\n",
      "Loss 1135: 0.0303227249532938\n",
      "Loss 1136: 0.03110427036881447\n",
      "Loss 1137: 0.030874017626047134\n",
      "Loss 1138: 0.03074226900935173\n",
      "Loss 1139: 0.03315700218081474\n",
      "Loss 1140: 0.030677037313580513\n",
      "Loss 1141: 0.030933380126953125\n",
      "Loss 1142: 0.030931100249290466\n",
      "Loss 1143: 0.0309948418289423\n",
      "Loss 1144: 0.0309783685952425\n",
      "Loss 1145: 0.03260282799601555\n",
      "Loss 1146: 0.03152349218726158\n",
      "Loss 1147: 0.03093641623854637\n",
      "Loss 1148: 0.030968492850661278\n",
      "Loss 1149: 0.030143484473228455\n",
      "Loss 1150: 0.030565891414880753\n",
      "Loss 1151: 0.029828188940882683\n",
      "Loss 1152: 0.03085021860897541\n",
      "Loss 1153: 0.030943775549530983\n",
      "Loss 1154: 0.029952313750982285\n",
      "Loss 1155: 0.029957899823784828\n",
      "Loss 1156: 0.02959551103413105\n",
      "Loss 1157: 0.031116219237446785\n",
      "Loss 1158: 0.02873787097632885\n",
      "Loss 1159: 0.029641466215252876\n",
      "Loss 1160: 0.02957896888256073\n",
      "Loss 1161: 0.029564805328845978\n",
      "Loss 1162: 0.02886638604104519\n",
      "Loss 1163: 0.030076898634433746\n",
      "Loss 1164: 0.03050696663558483\n",
      "Loss 1165: 0.031001783907413483\n",
      "Loss 1166: 0.03027687408030033\n",
      "Loss 1167: 0.029565386474132538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d47eaa757339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0md2wdx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0md2wdy2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0md2wdz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Git\\siren\\model\\nets.py\u001b[0m in \u001b[0;36mhessian\u001b[1;34m(self, x, i, j, k)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         inputs, allow_unused)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_steps = 5000\n",
    "\n",
    "optim = torch.optim.Adam(lr=1e-4, params=net.parameters())\n",
    "\n",
    "L = np.array(((-1,1),(-1,1),(0,8),(0,1)))\n",
    "ig = Integrator(Nd=4,L=L,Np=2**12)\n",
    "\n",
    "lam_div = 1\n",
    "lam_mm  = 1\n",
    "\n",
    "writer = SummaryWriter('runs/pipe_flowsup_ns1d-'+str(np.round(time.time())))\n",
    "\n",
    "for step in range(total_steps):\n",
    "    \n",
    "    \n",
    "#     func = lambda X: ((net(X)-pipe.fields(X))**2).sum(axis=1)\n",
    "\n",
    "#     dudxp = net.jacobian(X,0,0)\n",
    "#     dvdyp = net.jacobian(X,1,1)\n",
    "#     dwdzp = net.jacobian(X,2,2)\n",
    "    \n",
    "#     mseloss = ((F - Fp)**2).mean()\n",
    "#     divloss = ((dudxp+dvdyp+dwdzp)**2).mean()\n",
    "#     loss    = mseloss+lam*divloss\n",
    "    \n",
    "#     loss = ig.integrate(func,reltol=5e-3,maxiter=1e3)\n",
    "    \n",
    "    # Sample points\n",
    "    X = ig.points()\n",
    "    \n",
    "    # Fields\n",
    "    F = pipe.fields(X)\n",
    "    u = F[:,0]\n",
    "    v = F[:,2]\n",
    "    w = F[:,2]\n",
    "    \n",
    "    # First derivatives\n",
    "    dudx = net.jacobian(X,0,0)\n",
    "    dudy = net.jacobian(X,0,1)\n",
    "    dudz = net.jacobian(X,0,2)\n",
    "    dudt = net.jacobian(X,0,3)\n",
    "    \n",
    "    dvdx = net.jacobian(X,1,0)\n",
    "    dvdy = net.jacobian(X,1,1)\n",
    "    dvdz = net.jacobian(X,1,2)\n",
    "    dvdt = net.jacobian(X,1,3)\n",
    "    \n",
    "    dwdx = net.jacobian(X,2,0)\n",
    "    dwdy = net.jacobian(X,2,1)\n",
    "    dwdz = net.jacobian(X,2,2)\n",
    "    dwdt = net.jacobian(X,2,3)\n",
    "    \n",
    "    dpdx = net.jacobian(X,3,0)\n",
    "    dpdy = net.jacobian(X,3,1)\n",
    "    dpdz = net.jacobian(X,3,2)\n",
    "    \n",
    "    # Second derivatives\n",
    "    d2udx2 = net.hessian(X,0,0,0)\n",
    "    d2udy2 = net.hessian(X,0,1,1)\n",
    "    d2udz2 = net.hessian(X,0,2,2)\n",
    "    \n",
    "    d2vdx2 = net.hessian(X,1,0,0)\n",
    "    d2vdy2 = net.hessian(X,1,1,1)\n",
    "    d2vdz2 = net.hessian(X,1,2,2)\n",
    "    \n",
    "    d2wdx2 = net.hessian(X,2,0,0)\n",
    "    d2wdy2 = net.hessian(X,2,1,1)\n",
    "    d2wdz2 = net.hessian(X,2,2,2)\n",
    "    \n",
    "    # Momentum\n",
    "    mmx = dudx + u*dudx + v*dudy + w*dudz + dpdx - 1/Wo**2*(d2udx2 + d2udy2 + d2udz2)\n",
    "    mmy = dvdx + u*dvdx + v*dvdy + w*dvdz + dpdy - 1/Wo**2*(d2vdx2 + d2vdy2 + d2vdz2)\n",
    "    mmz = dwdx + u*dwdx + v*dwdy + w*dwdz + dpdz - 1/Wo**2*(d2wdx2 + d2wdy2 + d2wdz2)\n",
    "    \n",
    "    # Continuity\n",
    "    div = dudx + dvdy + dwdz\n",
    "    \n",
    "    # Loss Terms\n",
    "    mseloss = ((net(X)-F)**2).mean()\n",
    "    divloss = (div**2).mean()\n",
    "    mmloss  = (mmx**2+mmy**2+mmz**2).mean()\n",
    "    \n",
    "#     loss = mseloss + lam_div*divloss + lam_mm*mmloss\n",
    "    loss = mseloss + lam_div*divloss\n",
    "    \n",
    "    writer.add_scalar('MSE Loss',\n",
    "                        mseloss.item(),\n",
    "                        step)\n",
    "    writer.add_scalar('Divergence Loss',\n",
    "                        divloss.item(),\n",
    "                        step)\n",
    "    writer.add_scalar('Momentum Loss',\n",
    "                        mmloss.item(),\n",
    "                        step)\n",
    "    writer.add_scalar('Total Loss',\n",
    "                        loss.item(),\n",
    "                        step)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    print(\"Loss \" + str(step) + \": \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nxt = 32\n",
    "Nzt = 4*Nxt\n",
    "Ntt = 64\n",
    "\n",
    "xt,yt,zt,tt = np.meshgrid(np.linspace(-1,1,Nxt),np.linspace(0,0,1),np.linspace(0,4,Nzt),np.linspace(0,1,Ntt))\n",
    "ut,vt,wt = pipe.velocity(xt,yt,zt,tt)\n",
    "pt = pipe.pressure(xt,yt,zt,tt)\n",
    "ut = np.squeeze(ut[0,:,:,:])\n",
    "vt = np.squeeze(vt[0,:,:,:])\n",
    "wt = np.squeeze(wt[0,:,:,:])\n",
    "pt = np.squeeze(pt[0,:,:,:])\n",
    "\n",
    "\n",
    "Xt = np.concatenate((xt.reshape((Nxt*Nzt*Ntt,1)),yt.reshape((Nxt*Nzt*Ntt,1)),zt.reshape((Nxt*Nzt*Ntt,1)),tt.reshape((Nxt*Nzt*Ntt,1))),axis=1)\n",
    "Xt = torch.from_numpy(Xt).float().cuda()\n",
    "Fpt = net(Xt)\n",
    "\n",
    "Fpt = Fpt.cpu().detach().numpy().reshape((Nxt,1,Nzt,Ntt,4))\n",
    "upt = np.squeeze(Fpt[:,0,:,:,0])\n",
    "vpt = np.squeeze(Fpt[:,0,:,:,1])\n",
    "wpt = np.squeeze(Fpt[:,0,:,:,2])\n",
    "ppt = np.squeeze(Fpt[:,0,:,:,3])\n",
    "for tn in np.arange(Ntt):\n",
    "    fig, axes = plt.subplots(4,3, figsize=(16.5,5.5))\n",
    "    axes[0,0].imshow(ut[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[0,1].imshow(upt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[0,2].imshow(upt[:,:,tn]-ut[:,:,tn],vmin=-0.1,vmax=0.1)\n",
    "    \n",
    "    axes[1,0].imshow(vt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[1,1].imshow(vpt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[1,2].imshow(vpt[:,:,tn]-vt[:,:,tn],vmin=-0.1,vmax=0.1)\n",
    "    \n",
    "    axes[2,0].imshow(wt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[2,1].imshow(wpt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[2,2].imshow(wpt[:,:,tn]-wt[:,:,tn],vmin=-0.1,vmax=0.1)\n",
    "    \n",
    "    axes[3,0].imshow(pt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[3,1].imshow(ppt[:,:,tn],vmin=-1,vmax=1)\n",
    "    axes[3,2].imshow(ppt[:,:,tn]-pt[:,:,tn],vmin=-0.1,vmax=0.1)\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
